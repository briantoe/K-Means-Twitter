# -*- coding: utf-8 -*-
"""hw3_kmeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ufCoZIzMj7LRcqfbBObuQoa74CJNxPjR
"""

import re
from collections import Counter
import copy
import numpy as np

#Jaccard Distance Calculator 

#how do we feel about numbers? 

#this function will calculate the number of  similar words in two tweets 
#tweet1 ^ tweet2 
def calculateIntersection(tweet1, tweet2):
  tweet1_split = tweet1.split() #create an array of words for tweet1
  tweet2_split = tweet2.split() #create an array of words for tweet2

  #count the number of words that are similar that we have not seen before
  count = 0

  #keep track of the words we have already found to be similar 
  similar_words = ''

  #for every word on tweet1, check if in tweet2 and NOT in similar_words 
  for word in tweet1_split:
    if word in tweet2_split and not (word in similar_words):
     count = count + 1                        #increment count 
     similar_words = similar_words + word     #keep track of similar words 
  return count 

#this function will calculate the total number of unique words in two tweets
#tweet1 U tweet2  
def calculateUnion(tweet1, tweet2):
  tweet1_split = tweet1.split() #create an array of words for tweet1
  tweet2_split = tweet2.split() #create an array of words for tweet2

  #create a dictionary of all words in both string and how frequently it happens 
  count = Counter(tweet1_split + tweet2_split)
  #return the number of keys in the counter 
  return len(count.most_common())

#count the number of words in a tweet
#UNUSED 
def count_words(tweet):
  split_string = tweet.split()
  word_count = len(split_string)
  return word_count   

#calculate the jaccard distance between two tweets 
def calculate_jaccard_dist(tweet1, tweet2):
  #calculate the union
  union = calculateUnion(tweet1, tweet2)
  #calculate the intersection 
  intersection = calculateIntersection(tweet1, tweet2)
  #return the jaccard distance 
  return 1 - intersection/union

def process_tweet(tweet):
  tweet = tweet.lower()  # convert every word to lowercase
  no_urls = re.sub(r'http\S+', '', tweet).strip()[50:]  # remove any urls, remove tweet id and timestamp
  no_handles = re.sub('@[^\s]+','', no_urls)  # removes @handles
  processed = re.sub('#', '', no_handles) #  removes hashtags here
  return processed.rstrip() #return the strings with all words in the tweet

def open_txt(filename):
  file_lines = []
  with open(filename, 'r') as f:
    file_lines = f.readlines()

  return file_lines

def compute_dist_sum(tweet, other_tweets):
  dists = []
  for other_tweet in other_tweets:
    dists.append(calculate_jaccard_dist(tweet, other_tweet))
  
  return sum(dists)

class Kmeans():
  def __init__(self):
    self.tweets = None
    self.means = None
    self.clusters = None
    self.sse = None

  def import_data(self, filename):
    tweets = open_txt(filename)
    self.tweets = [process_tweet(tweet) for tweet in tweets]
    
  def compute_objective(self):
    dist = 0
    for key in self.clusters:
        dist += sum([calculate_jaccard_dist(tweet, self.means[key]) ** 2 for tweet in self.clusters[key]]) # jaccards distance here?

    return dist

  def compute_kmeans(self, k):
    means = [] 
    clusters = dict() 
        
    # randomly assign positions for k centroids of clusters
    for _ in range(k):
      rand_centroid = np.random.randint(0, len(self.tweets) - 1)
      means.append(self.tweets[rand_centroid])

    # keep track of previous means, if there's no change between the current means and previous means then the algorithm is converged
    previous_means = None
    while True:
      clusters = dict() # S 
      # initialize the buckets for S
      for i in range(k):
          clusters[i] = []

      for datapoint in self.tweets: # iterate through each datapoint
        dists = [] 
        for mean in means: # calculate distances^2 for all means and store the minimum value
          dists.append(calculate_jaccard_dist(datapoint, mean) ** 2)
          # dists.append(euclidean(datapoint, mean) ** 2) # jaccards distance goes here?
        
        m = min(dists)
        # the index of the minimum distance can access the ith cluster (bucket) to put the datapoint into
        clusters[dists.index(m)].append(datapoint)

      for key in clusters: # iterate through all clusters (buckets)
        if len(clusters[key]) == 0: # if there's nothing in the bucket, ignore it
          continue
        else:   # compute the "centroid" by choosing the tweet that has minimum distance from all other tweets
          # means[key] = np.mean(clusters[key], axis=0) numerical version
          minimum = np.inf
          tweet_bucket = clusters[key]
          for tweet in tweet_bucket:
            temp = compute_dist_sum(tweet, tweet_bucket)
            if temp < minimum:
              means[key] = tweet
              minimum = temp

      if np.array_equal(means, previous_means): # if the update didn't change anything, k-means has converged
        break 
      previous_means = copy.deepcopy(means)

    self.means = means
    self.clusters = clusters
    self.sse = self.compute_objective()
    return means, clusters, self.sse

"""# Main Method"""

kmeans = Kmeans()
kmeans.import_data('cbchealth.txt')

means, clusters, sse = kmeans.compute_kmeans(k=5)
print('k = 5')
print('sse =', sse)
for c in clusters:
  print(str(c + 1) + ": " + str(len(clusters[c])))

means, clusters, sse = kmeans.compute_kmeans(k=10)
print('k = 10')
print('sse =', sse)
for c in clusters:
  print(str(c + 1) + ": " + str(len(clusters[c])))

means, clusters, sse = kmeans.compute_kmeans(k=15)
print('k = 15')
print('sse =', sse)
for c in clusters:
  print(str(c + 1) + ": " + str(len(clusters[c])))

means, clusters, sse = kmeans.compute_kmeans(k=20)
print('k = 20')
print('sse =', sse)
for c in clusters:
  print(str(c + 1) + ": " + str(len(clusters[c])))

means, clusters, sse = kmeans.compute_kmeans(k=30)
print('k = 30')
print('sse =', sse)
for c in clusters:
  print(str(c + 1) + ": " + str(len(clusters[c])))

means, clusters, sse = kmeans.compute_kmeans(k=50)
print('k = 50')
print('sse =', sse)
for c in clusters:
  print(str(c + 1) + ": " + str(len(clusters[c])))

means, clusters, sse = kmeans.compute_kmeans(k=100)
print('k = 100')
print('sse =', sse)
for c in clusters:
  print(str(c + 1) + ": " + str(len(clusters[c])))

means, clusters, sse = kmeans.compute_kmeans(k=len(kmeans.tweets)-1)
print('k =', len(kmeans.tweets))
print('sse =', sse)
# for c in clusters:
#   print(str(c + 1) + ": " + str(len(clusters[c])))